# @package _global_
training:
  # Optimization parameters
  learning_rate: 1e-4
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  epsilon: 1e-8
  
  # Training schedule
  num_epochs: 10
  warmup_steps: 1000
  max_grad_norm: 1.0
  
  # Evaluation
  eval_every_n_steps: 500
  save_every_n_steps: 1000
  
  # Loss parameters
  label_smoothing: 0.0
  
  # Compilation
  compile_train_step: true
  compile_eval_step: true
  
  # Logging
  log_every_n_steps: 100
  
  # Checkpointing
  save_checkpoints: true
  checkpoint_dir: ./checkpoints
  max_checkpoints_to_keep: 3